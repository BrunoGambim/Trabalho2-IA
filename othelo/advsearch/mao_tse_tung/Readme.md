Bruno Reck Gambim, 00314928, Turma B
Leonardo Reis da Silva, 00316357, Turma A
Bruno da Costa Sauner, 00313146, Turma A

Nossa estratégia de parada é o iterative deepening. Para aproveitar parte do resultado das iterações anteriores, nós optamos por salvar o estado do board em uma tabela hash. Junto com o estado do board, salvamos os movimentos possíveis em uma lista pseudo ordenada, sempre que um movimento de um estado é melhor que o melhor movimento atual, ele é puxado para o começo da lista. Com essa otimização, conseguimos aumentar o número de podas. Testamos manter a lista sempre completamente ordenada com base nas iterações anteriores, mas o overhead para manter a lista ordenada acaba superando os benefícios de fazer a operação.
Outra otimização no alfa beta pruning  foi criar um modo de fim de jogo para quando já é possível alcançar os estados terminais. Nas rodadas finais do jogo, ao invés de usar o iterative deepening, rodamos o alfa beta pruning que só pára quando alcança um estado terminal. Caso o estado terminal seja uma vitória, a função de avaliação retorna um valor muito alto, caso seja uma derrota, ele retorna um valor muito pequeno, caso seja um empate, ele retorna 0. Pegamos a ideia de criar um modo de fim de jogo de [2]. Como não conseguimos pensar em um método melhor para estimar em qual rodada já é possível alcançar os estados terminais, encontramos um valor experimentalmente e deixamos ele como valor fixo.
Nós criamos nossa função de avaliação nos baseando em uma das heurísticas de [1]. Para conseguir criar uma função de avaliação relativamente complexa sem gerar um overhead proibitivo, salvamos a função de avaliação em 10 tabelas hash. Cada uma dessas 10 tabelas corresponde a uma linha do jogo com mais de 3 quadrados na vertical, horizontal ou diagonal. Seriam ao todo 38 tabelas ( 8 na horizontal, 8 na vertical e 22 na diagonal), mas reduzimos para 10 usando a simetria do campo. Embora sejam 10 tabelas, na hora da consulta para avaliar o estado do campo, algumas tabelas são consultadas mais de uma vez, somando a um total de 38 consultas. Cada linha do campo recebe um valor, penalizando ( ou bonificando) jogadores por peças consecutivas, avaliando as peças pela sua posição no campo e penalizando peças pelo peso dos quadrados adjacentes vazios. Por fim, nos baseamos em [2] para, no caso das linhas que representam as laterais do campo, bonificar jogadores que possuam um canto por cada peça conectada ao canto, pois, peças nas laterais conectadas ao canto não podem ser dominadas caso o jogador possua o canto.
Por fim, nossa função que compila a tabela usa ao todo 13 variáveis, 10 para o pesos das posições do campo (usamos a simetria do campo para reduzir de 64 para 10), 3 para os coeficientes de cada da heurística. Para otimizar o valor dessas variáveis, utilizamos um sistema de aprendizado baseado em torneios e do algoritmo CMA-ES. Como não era o foco do trabalho, pegamos o algoritmo de CMA-ES pronto de [3]. Antes de começar a etapa de aprendizado, definimos alguns agentes fixos, usando versões anteriores do agente ou mesma versão do agente com pesos pré definidos para os coeficientes da heurística. Após isso, começamos uma etapa de torneios, onde cada geração do algoritmo CMA-ES possui um conjunto de listas de variáveis para os agentes que vão participar de um torneio. No total, fora os agentes fixos, cada torneio possui 11 participantes. Cada geração corresponde a um torneio. Para cada torneio são gerados alguns campos diferentes, gerados através de um número x de jogadas aleatórias a partir do board original. Cada um desses campos tem um peso associado a ele (esse peso diminui proporcionalmente ao número de jogadas realizadas para criá-lo). Cada um dos 11 participantes joga os dois lados de cada mapa contra os agentes fixo, ganhando 2 vezes o peso do campo pontos por vitória, o peso do campo pontos por empate e 0 pontos por derrota. Além dos pontos por vitória, os participantes ganham 1 ponto por cada canto dominado na partida. No fim do torneio, o número de pontos de cada participante é informado ao algoritmo do CMA-ES que pode gerar a nova geração. Caso um player ganhe quase todas as partidas, nós parávamos o programa manualmente, iniciamos o novamente mudando a lista de variáveis do agente fixo para a lista desse player vitorioso. Nós pegamos a ideia de sistema de torneio de [2].
	Como não estava especificado, não enviamos os códigos usados do torneio nem os códigos usados para gerar as tabelas junto com o trabalho, mas eles podem ser acessados em [4].

Referências:
[1]https://matthieu-zimmer.net/~matthieu/courses/python/othello.pdf
[2]https://www.researchgate.net/publication/221024307_A_Genetic_Algorithm_to_Improve_an_Othello_Program
[3]https://pypi.org/project/cma/
[4]https://github.com/BrunoGambim/Trabalho2-IA
